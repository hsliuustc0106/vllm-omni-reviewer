# PR #1339: [Feature] Support FSDP for diffusion models

## Summary

**Author:** gcanlin | **Closes:** #1326 | **Files changed:** 14

Adds HSDP (Hybrid Sharded Data Parallel) support for diffusion model inference using PyTorch FSDP2. ~25GB memory savings per GPU on Wan2.2-14B (80GB to 55GB on A100s).

### Architecture
- DiffusionParallelConfig: new HSDP fields + validation
- HSDPInferenceConfig: runtime dataclass
- apply_hsdp_to_model / shard_model: core sharding via fully_shard
- _hsdp_shard_conditions: per-model attribute for shardable modules
- _load_model_with_hsdp: CPU init -> load weights -> shard
- execute_model: conditional no_grad vs inference_mode

### Blocking Issues
1. assert for user-facing validation in data.py (stripped under python -O)
2. _is_transformer_block shard condition too broad
3. _load_model_with_hsdp missing custom_pipeline support

### Non-blocking
- requires_grad=False after FSDP may be redundant (no_grad context covers it)
- Breaking change: removed old hsdp_replicate_dim/hsdp_shard_dim from OmniDiffusionConfig
- No integration test for actual sharding path
- PR title says FSDP but implementation is HSDP

### Verdict: Needs minor fixes before merge

